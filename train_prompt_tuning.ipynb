{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600f9a46-8428-4f34-85fe-5472c8ae5f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Minimal PEFT soft prompt tuning example for SST-2 using t5-small.\n",
    "Uses a SimpleSeq2SeqCollator that removes any stray 'label' key (scalar class)\n",
    "and only provides 'labels' (padded tensor) to the model.\n",
    "\n",
    "Supports:\n",
    " - discrete baseline (--mode discrete)\n",
    " - prompt tuning (--mode prompt)\n",
    "\n",
    "Outputs saved in ./outputs/<run_id>\n",
    "\n",
    "Example:\n",
    " python train_prompt_tuning.py --model_name t5-small --mode prompt --num_virtual_tokens 20 --subset 2000 --num_train_epochs 3\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")\n",
    "from peft import PromptTuningConfig, get_peft_model, TaskType\n",
    "\n",
    "# ----------------------------\n",
    "# Utilities\n",
    "# ----------------------------\n",
    "def parse_args():\n",
    "    p = argparse.ArgumentParser()\n",
    "    p.add_argument(\"--model_name\", type=str, default=\"t5-small\")\n",
    "    p.add_argument(\"--mode\", type=str, default=\"prompt\", choices=[\"discrete\", \"prompt\"])\n",
    "    p.add_argument(\"--num_virtual_tokens\", type=int, default=20)\n",
    "    p.add_argument(\"--output_dir\", type=str, default=\"outputs\")\n",
    "    p.add_argument(\"--per_device_train_batch_size\", type=int, default=8)\n",
    "    p.add_argument(\"--per_device_eval_batch_size\", type=int, default=16)\n",
    "    p.add_argument(\"--learning_rate\", type=float, default=1e-3)\n",
    "    p.add_argument(\"--num_train_epochs\", type=int, default=3)\n",
    "    p.add_argument(\"--max_input_length\", type=int, default=128)\n",
    "    p.add_argument(\"--max_target_length\", type=int, default=4)\n",
    "    p.add_argument(\"--seed\", type=int, default=42)\n",
    "    p.add_argument(\"--num_seeds\", type=int, default=1)\n",
    "    p.add_argument(\"--subset\", type=int, default=0, help=\"limit train set; 0 means full\")\n",
    "    p.add_argument(\"--no_cuda\", action=\"store_true\", help=\"force CPU\")\n",
    "    p.add_argument(\"--debug\", action=\"store_true\", help=\"enable debug output\")\n",
    "    return p.parse_args()\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea06ee3f-6458-46ed-9cf3-473ae8272c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Preprocessing\n",
    "# ----------------------------\n",
    "def preprocess_batch(examples: Dict[str, List], tokenizer: AutoTokenizer, max_input_length: int, max_target_length: int):\n",
    "    \"\"\"\n",
    "    Tokenize inputs and targets, return dict suitable for dataset.map (batched=True).\n",
    "    Ensures labels are list-of-lists and pad token ids are replaced with -100.\n",
    "    \"\"\"\n",
    "    # Ensure pad token exists\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    inputs = [\"sst2 sentence: \" + s for s in examples[\"sentence\"]]\n",
    "    targets = [\"positive\" if l == 1 else \"negative\" for l in examples[\"label\"]]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        targets,\n",
    "        max_length=max_target_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "    normalized_labels = []\n",
    "    for lab in labels[\"input_ids\"]:\n",
    "        # coerce numpy / torch / scalar -> list\n",
    "        if isinstance(lab, np.ndarray):\n",
    "            lab = lab.tolist()\n",
    "        if isinstance(lab, torch.Tensor):\n",
    "            lab = lab.tolist()\n",
    "        if isinstance(lab, int):\n",
    "            lab = [lab]\n",
    "        if not isinstance(lab, (list, tuple)):\n",
    "            try:\n",
    "                lab = list(lab)\n",
    "            except Exception:\n",
    "                lab = [-100]\n",
    "        # coerce elements to ints and replace pad with -100\n",
    "        lab_ints = [(int(tok) if tok != pad_id else -100) for tok in lab]\n",
    "        normalized_labels.append(lab_ints)\n",
    "\n",
    "    model_inputs[\"labels\"] = normalized_labels\n",
    "    return model_inputs\n",
    "\n",
    "def generate_dataset_splits(tokenizer: AutoTokenizer, args) -> tuple[Dict[str, Any], Dataset, Dataset, Dataset]:\n",
    "    ds = load_dataset(\"glue\", \"sst2\")\n",
    "    if args.subset and args.subset > 0:\n",
    "        ds[\"train\"] = ds[\"train\"].select(range(min(args.subset, len(ds[\"train\"]))))\n",
    "    tokenized_train = ds[\"train\"].map(lambda ex: preprocess_batch(ex, tokenizer, args.max_input_length, args.max_target_length), batched=True)\n",
    "    tokenized_val = ds[\"validation\"].map(lambda ex: preprocess_batch(ex, tokenizer, args.max_input_length, args.max_target_length), batched=True)\n",
    "    tokenized_test = ds[\"test\"].map(lambda ex: preprocess_batch(ex, tokenizer, args.max_input_length, args.max_target_length), batched=True)\n",
    "\n",
    "    # Final defensive per-row normalization: ensure labels are python lists and remove raw 'label' if present\n",
    "    def _fix_row(row):\n",
    "        lab = row.get(\"labels\", None)\n",
    "        if lab is None:\n",
    "            # if labels missing but 'label' (scalar) exists, create labels from it\n",
    "            if \"label\" in row:\n",
    "                scalar = row[\"label\"]\n",
    "                try:\n",
    "                    row[\"labels\"] = [int(scalar)]\n",
    "                except Exception:\n",
    "                    row[\"labels\"] = [-100]\n",
    "                # optionally remove the scalar to avoid being forwarded to model\n",
    "                row.pop(\"label\", None)\n",
    "                return row\n",
    "            return row\n",
    "        # elementwise coercion\n",
    "        if isinstance(lab, (np.ndarray,)):\n",
    "            lab = lab.tolist()\n",
    "        if isinstance(lab, (int, np.integer)):\n",
    "            lab = [int(lab)]\n",
    "        if isinstance(lab, (list, tuple)):\n",
    "            lab = [int(x) if not (x is None) else -100 for x in lab]\n",
    "        else:\n",
    "            try:\n",
    "                lab_list = list(lab)\n",
    "                lab = [int(x) for x in lab_list]\n",
    "            except Exception:\n",
    "                lab = [-100]\n",
    "        # replace pad ids in-case any slipped through (pad_id should exist)\n",
    "        pad_id = tokenizer.pad_token_id\n",
    "        if pad_id is not None:\n",
    "            lab = [(tok if tok != pad_id else -100) for tok in lab]\n",
    "        row[\"labels\"] = lab\n",
    "        # ensure raw scalar 'label' doesn't exist\n",
    "        row.pop(\"label\", None)\n",
    "        return row\n",
    "\n",
    "    tokenized_train = tokenized_train.map(_fix_row, batched=False)\n",
    "    tokenized_val = tokenized_val.map(_fix_row, batched=False)\n",
    "    tokenized_test = tokenized_test.map(_fix_row, batched=False)\n",
    "    return ds, tokenized_train, tokenized_val, tokenized_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879193f9-3282-4e52-bef9-fd66572c293c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Simple collator\n",
    "# ----------------------------\n",
    "class SimpleSeq2SeqCollator:\n",
    "    \"\"\"\n",
    "    Simple collator for seq2seq tasks that:\n",
    "    - pads inputs via tokenizer.pad(...)\n",
    "    - pads labels manually into a LongTensor and converts pad_token_id -> -100\n",
    "    - removes any stray 'label' scalar from features so the model never receives unexpected kwargs\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer: AutoTokenizer, pad_to_multiple_of: Optional[int] = None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad_to_multiple_of = pad_to_multiple_of\n",
    "\n",
    "    def _coerce_to_list_of_ints(self, lab) -> List[int]:\n",
    "        # handle torch.Tensor / np.ndarray / numpy scalars / int / list / tuple / generator\n",
    "        if isinstance(lab, torch.Tensor):\n",
    "            lab = lab.cpu().numpy()\n",
    "        if isinstance(lab, np.ndarray):\n",
    "            # 0-d array yields shape == ()\n",
    "            if lab.shape == ():\n",
    "                return [int(lab.item())]\n",
    "            try:\n",
    "                lab_list = lab.tolist()\n",
    "            except Exception:\n",
    "                return [-100]\n",
    "            lab = lab_list\n",
    "        if isinstance(lab, (np.integer,)):\n",
    "            return [int(lab)]\n",
    "        if isinstance(lab, int):\n",
    "            return [int(lab)]\n",
    "        if isinstance(lab, str):\n",
    "            # unexpected: treat as bad -> fallback\n",
    "            return [-100]\n",
    "        # Try to convert iterable -> list\n",
    "        try:\n",
    "            candidate = list(lab)\n",
    "        except Exception:\n",
    "            try:\n",
    "                return [int(lab)]\n",
    "            except Exception:\n",
    "                return [-100]\n",
    "        out = []\n",
    "        for el in candidate:\n",
    "            try:\n",
    "                if isinstance(el, (np.integer,)):\n",
    "                    out.append(int(el))\n",
    "                else:\n",
    "                    out.append(int(el))\n",
    "            except Exception:\n",
    "                out.append(-100)\n",
    "        return out\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        # prepare input features for tokenizer.pad (exclude both 'labels' and raw 'label')\n",
    "        features_no_label_keys = [{k: v for k, v in f.items() if k not in (\"labels\", \"label\")} for f in features]\n",
    "        batch = self.tokenizer.pad(\n",
    "            features_no_label_keys,\n",
    "            padding=\"longest\",\n",
    "            return_tensors=\"pt\",\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "        )\n",
    "\n",
    "        # Build label sequences as tensors (coerce from either 'labels' or scalar 'label')\n",
    "        pad_id = self.tokenizer.pad_token_id\n",
    "        label_tensors = []\n",
    "        for f in features:\n",
    "            # prefer 'labels' (tokenized target ids). If absent, fall back to 'label' scalar (class)\n",
    "            lab = f.get(\"labels\", None)\n",
    "            if lab is None and \"label\" in f:\n",
    "                lab = f[\"label\"]\n",
    "            if lab is None:\n",
    "                label_tensors.append(torch.tensor([], dtype=torch.long))\n",
    "                continue\n",
    "            lab_list = self._coerce_to_list_of_ints(lab)\n",
    "            label_tensors.append(torch.tensor(lab_list, dtype=torch.long))\n",
    "\n",
    "        if len(label_tensors) > 0:\n",
    "            padded = pad_sequence(label_tensors, batch_first=True, padding_value=(pad_id if pad_id is not None else 0))\n",
    "            # convert pad token id to -100 for loss ignore\n",
    "            if pad_id is not None:\n",
    "                padded[padded == pad_id] = -100\n",
    "            else:\n",
    "                padded[padded == 0] = -100\n",
    "            batch[\"labels\"] = padded\n",
    "        else:\n",
    "            batch[\"labels\"] = torch.zeros((len(features), 0), dtype=torch.long)\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a463fdb-1400-4da2-ae2f-5793d14329bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Baseline & training\n",
    "# ----------------------------\n",
    "def run_discrete_baseline(tokenizer: AutoTokenizer, dataset: Dict[str, Any], args):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(args.model_name).to(device)\n",
    "\n",
    "    def gen_and_score(split):\n",
    "        inputs = [\"sst2 sentence: \" + s for s in split[\"sentence\"]]\n",
    "        enc = tokenizer(inputs, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        enc = {k: v.to(device) for k, v in enc.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**enc, max_length=args.max_target_length)\n",
    "        preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        labels = [\"positive\" if l == 1 else \"negative\" for l in split[\"label\"]]\n",
    "        acc = sum(1 if p.strip() == l.strip() else 0 for p, l in zip(preds, labels)) / len(preds)\n",
    "        return acc, None\n",
    "\n",
    "    return gen_and_score(dataset[\"validation\"]), gen_and_score(dataset[\"test\"])\n",
    "\n",
    "def train_prompt_tuning(tokenizer: AutoTokenizer, ds, tokenized_train, tokenized_val, tokenized_test, args, run_id):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(args.model_name).to(device)\n",
    "\n",
    "    # configure prompt tuning (PEFT)\n",
    "    prompt_config = PromptTuningConfig(task_type=TaskType.SEQ_2_SEQ_LM, num_virtual_tokens=args.num_virtual_tokens)\n",
    "    model = get_peft_model(model, prompt_config)\n",
    "\n",
    "    data_collator = SimpleSeq2SeqCollator(tokenizer)\n",
    "\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=f\"{args.output_dir}/{run_id}\",\n",
    "        per_device_train_batch_size=args.per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=args.per_device_eval_batch_size,\n",
    "        do_train=True,\n",
    "        do_eval=True,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=200,\n",
    "        num_train_epochs=args.num_train_epochs,\n",
    "        learning_rate=args.learning_rate,\n",
    "        weight_decay=0.0,\n",
    "        save_total_limit=2,\n",
    "        predict_with_generate=False,\n",
    "        fp16=False,\n",
    "        report_to=\"none\",\n",
    "        seed=args.seed,\n",
    "    )\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_val,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(f\"{args.output_dir}/{run_id}\")\n",
    "\n",
    "    # Evaluate via generation (simple)\n",
    "    def generate_and_score(split):\n",
    "        model.eval()\n",
    "        accs = []\n",
    "        for i in range(0, len(split), args.per_device_eval_batch_size):\n",
    "            batch = split[i : i + args.per_device_eval_batch_size]\n",
    "            inputs = tokenizer([\"sst2 sentence: \" + s for s in batch[\"sentence\"]], truncation=True, padding=True, return_tensors=\"pt\")\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(**inputs, max_length=args.max_target_length)\n",
    "            preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            labels = [\"positive\" if l == 1 else \"negative\" for l in batch[\"label\"]]\n",
    "            for p, l in zip(preds, labels):\n",
    "                accs.append(1 if p.strip() == l.strip() else 0)\n",
    "        acc = sum(accs) / len(accs)\n",
    "        ece = 0.0  # placeholder: computing ECE requires per-example confidence extraction\n",
    "        return acc, ece\n",
    "\n",
    "    val_acc, val_ece = generate_and_score(ds[\"validation\"])\n",
    "    test_acc, test_ece = generate_and_score(ds[\"test\"])\n",
    "    return {\"val_acc\": val_acc, \"val_ece\": val_ece, \"test_acc\": test_acc, \"test_ece\": test_ece, \"out_dir\": f\"{args.output_dir}/{run_id}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856e4891-c75a-4177-8946-c9ae90ff8cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Main\n",
    "# ----------------------------\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    for seed_offset in range(args.num_seeds):\n",
    "        current_seed = args.seed + seed_offset\n",
    "        set_seed(current_seed)\n",
    "\n",
    "        timestamp = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "        run_id = f\"run_{args.mode}_{args.model_name.replace('/', '_')}_vt{args.num_virtual_tokens}_s{current_seed}_{timestamp}\"\n",
    "        Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        print(f\"\\n=== RUN {run_id} (seed {current_seed}) ===\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        ds, tokenized_train, tokenized_val, tokenized_test = generate_dataset_splits(tokenizer, args)\n",
    "\n",
    "        if args.debug:\n",
    "            # write a few label-type samples for inspection\n",
    "            dbg_dir = Path(args.output_dir) / run_id\n",
    "            dbg_dir.mkdir(parents=True, exist_ok=True)\n",
    "            def sample_labels(tok_dataset, name, n=20):\n",
    "                entries = []\n",
    "                for i in range(min(n, len(tok_dataset))):\n",
    "                    lab = tok_dataset[i].get(\"labels\", None)\n",
    "                    entries.append({\"idx\": i, \"type\": type(lab).__name__, \"repr\": lab if isinstance(lab, list) and len(str(lab)) < 200 else str(type(lab))})\n",
    "                with open(dbg_dir / f\"label_types_{name}.json\", \"w\") as f:\n",
    "                    json.dump(entries, f, indent=2)\n",
    "            sample_labels(tokenized_train, \"train\")\n",
    "            sample_labels(tokenized_val, \"val\")\n",
    "            sample_labels(tokenized_test, \"test\")\n",
    "            print(f\"[debug] wrote label_types files to {dbg_dir}\")\n",
    "\n",
    "        if args.mode == \"discrete\":\n",
    "            (val_metrics, _), (test_metrics, _) = run_discrete_baseline(tokenizer, ds, args)\n",
    "            result = {\"run_id\": run_id, \"val_acc\": val_metrics, \"test_acc\": test_metrics}\n",
    "        elif args.mode == \"prompt\":\n",
    "            metrics = train_prompt_tuning(tokenizer, ds, tokenized_train, tokenized_val, tokenized_test, args, run_id)\n",
    "            result = {\"run_id\": run_id, **metrics}\n",
    "        else:\n",
    "            raise ValueError(\"unknown mode\")\n",
    "\n",
    "        # save summary\n",
    "        out_dir = Path(args.output_dir) / run_id\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "        with open(out_dir / \"summary.json\", \"w\") as f:\n",
    "            json.dump(result, f, indent=2)\n",
    "\n",
    "        print(\"Result:\", result)\n",
    "\n",
    "    print(\"\\n=== ALL RUNS COMPLETE ===\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
